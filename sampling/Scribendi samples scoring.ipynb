{
 "cells": [
  {
   "cell_type": "raw",
   "id": "8d1bdda8-5a48-4cb0-8c98-df34e7bcc03d",
   "metadata": {},
   "source": [
    "!pip install fuzzywuzzy==0.18.0 -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9251a2-403d-4086-bc88-fe817dab9e88",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8799df70-911c-41b7-99d6-730a9cd7f214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "from fuzzywuzzy.fuzz import token_sort_ratio\n",
    "import torch\n",
    "import argparse\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class ScribendiScore:\n",
    "    def __init__(self, \n",
    "        threshold: float=0.8,\n",
    "        model_id: str='gpt2',\n",
    "        no_cuda: bool=False\n",
    "    ) -> None:\n",
    "        self.threshold = threshold\n",
    "        self.model_id = model_id\n",
    "        self.no_cuda = no_cuda\n",
    "        self.tokenizer, self.model = self.load_model(model_id)\n",
    "    \n",
    "    def score(self,\n",
    "        src_sents: List[str],\n",
    "        pred_sents: List[str],\n",
    "        batch_size: int=32,\n",
    "        verbose: bool=False\n",
    "    ) -> int:\n",
    "        src_sents, pred_sents, count = self.remove_eq_sents(src_sents, pred_sents)\n",
    "        src_ppls = self.ppl(src_sents, batch_size)\n",
    "        pred_ppls = self.ppl(pred_sents, batch_size)\n",
    "        score = 0\n",
    "        score2freq = {-1:0, 0:count, 1:0}\n",
    "        for i, (src, pred) in enumerate(zip(src_sents, pred_sents)):\n",
    "            if src_ppls[i] <= pred_ppls[i]:\n",
    "                score += -1\n",
    "                score2freq[-1] += 1\n",
    "                continue\n",
    "            tsr = self.token_sort_ratio(src, pred)\n",
    "            ldr = self.levenshtein_distance_ratio(src, pred)\n",
    "            if max(tsr, ldr) >= self.threshold:\n",
    "                score += 1\n",
    "                score2freq[1] += 1\n",
    "            else:\n",
    "                score += -1\n",
    "                score2freq[-1] += 1\n",
    "        #print('score2freq ->', score2freq, ', score ->', score2freq[1] - score2freq[-1])\n",
    "        return score\n",
    "                \n",
    "    def ppl(self, sents: List[str], batch_size: int=32) -> List[int]:\n",
    "        ppls = []\n",
    "        sents = [self.tokenizer.bos_token + sent for sent in sents]\n",
    "        for i in range(len(sents)//batch_size+1):\n",
    "            batch = sents[i*batch_size:(i+1)*batch_size]\n",
    "            if len(batch) == 0:\n",
    "                continue\n",
    "            inputs = self.tokenizer(batch, return_tensors='pt', padding=True)\n",
    "            if not self.no_cuda:\n",
    "                inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(\n",
    "                    inputs['input_ids'],\n",
    "                    attention_mask=inputs['attention_mask'],\n",
    "                    labels=inputs['input_ids']\n",
    "                )\n",
    "                shift_logits = outputs.logits[:, :-1, :].contiguous()\n",
    "                shift_labels = inputs['input_ids'][:, 1:].contiguous()\n",
    "                shift_mask = inputs['attention_mask'][:, 1:].contiguous()\n",
    "                batch_size, seq_len = shift_labels.shape\n",
    "                loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "                loss = loss_fn(\n",
    "                    shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                    shift_labels.view(-1)\n",
    "                ).view(batch_size, seq_len)\n",
    "                loss = (loss * shift_mask).sum(dim=1) / shift_mask.sum(dim=1)\n",
    "                ppls += torch.exp(loss).tolist()\n",
    "        return ppls\n",
    "\n",
    "    @staticmethod\n",
    "    def token_sort_ratio(src: str, pred: str) -> float:\n",
    "        return token_sort_ratio(src, pred) / 100\n",
    "    \n",
    "    @staticmethod\n",
    "    def levenshtein_distance_ratio(src: str, pred: str) -> float:\n",
    "        len_src = len(src)\n",
    "        len_pred = len(pred)\n",
    "        dp = [[0]*(len_pred+1) for _ in range(len_src+1)]\n",
    "        # dp = np.zeros((len_src+1, len_pred+1))\n",
    "        for i in range(1, len_src+1):\n",
    "            dp[i][0] = i\n",
    "        for j in range(1, len_pred+1):\n",
    "            dp[0][j] = j\n",
    "        for i in range(1, len_src+1):\n",
    "            for j in range(1, len_pred+1):\n",
    "                cost = 0\n",
    "                if src[i-1] != pred[j-1]:\n",
    "                    cost = 2 # Replacement cost is 2\n",
    "                dp[i][j] = min(\n",
    "                    dp[i-1][j-1] + cost,\n",
    "                    min(dp[i-1][j] + 1, dp[i][j-1] + 1)\n",
    "                )\n",
    "        return 1 - dp[len_src][len_pred] / (len_src + len_pred)\n",
    "\n",
    "    def load_model(self, \n",
    "        model_id: str\n",
    "    ) -> Tuple[GPT2TokenizerFast, GPT2LMHeadModel]:\n",
    "        local=os.path.exists(model_id)\n",
    "        tokenizer = GPT2TokenizerFast.from_pretrained(model_id,\n",
    "                local_files_only=local)\n",
    "        model = GPT2LMHeadModel.from_pretrained(model_id,\n",
    "                local_files_only=local)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        if not self.no_cuda:\n",
    "            model.to('cuda')\n",
    "        return tokenizer, model\n",
    "        \n",
    "    @staticmethod\n",
    "    def remove_eq_sents(\n",
    "        src_sents: List[str],\n",
    "        pred_sents: List[str]\n",
    "    )-> Tuple[List[str], List[str], int]:\n",
    "        new_src_sents = []\n",
    "        new_pred_sents = []\n",
    "        count = 0\n",
    "        for src, pred in zip(src_sents, pred_sents):\n",
    "            if src != pred:\n",
    "                new_src_sents.append(src)\n",
    "                new_pred_sents.append(pred)\n",
    "            else:\n",
    "                count += 1\n",
    "        return new_src_sents, new_pred_sents, count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cb9ccd-62c2-45ae-b3d3-19062c25cc65",
   "metadata": {},
   "source": [
    "## Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0763aa5-f517-4eb7-bb57-8bfdf32255b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"gpt2\"\n",
    "threshold = 0.8\n",
    "no_cuda = True\n",
    "\n",
    "scorer = ScribendiScore(\n",
    "    model_id=model_id,\n",
    "    threshold=threshold,\n",
    "    no_cuda=no_cuda\n",
    ")\n",
    "src = [\"Once the test is done , whether the results should be open to his or her relatives has caused social extensive controversy.\"]\n",
    "pred = [\"Once the test is done , whether the results should be open to his or her relatives has caused extensive social controversy.\"]\n",
    "print('src:', src)\n",
    "print('pred:', pred)\n",
    "print('ppl of src:', scorer.ppl(src)) # [198.90069580078125] Note: Cannot be reproduced\n",
    "print('ppl of pred:', scorer.ppl(pred)) # [119.57299041748047] Note: Cannot be reproduced\n",
    "print('levenshtein distance ratio:', scorer.levenshtein_distance_ratio(src[0], pred[0])) # 0.94308\n",
    "print('token sort ratio:', scorer.token_sort_ratio(src[0], pred[0])) # 1.0\n",
    "print('scribendi score:', scorer.score(src, pred)) # 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06cde81-151b-48ec-9951-1c42cb37684b",
   "metadata": {},
   "source": [
    "# Generate Preference dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d38a955-815a-459f-8705-6140424a8b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46bda3b-3b45-4beb-9c08-cf6d7a138272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7b794c-db5c-4b18-a3b8-4716eb91bf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('troy-blogs.train.tokenized.csv')\n",
    "df_sample = df.sample(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aef26a-7b4c-4a2c-b2f4-32a5ee14ef32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"token-abc123\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf42aec-a8ab-44b0-96c9-58406ab82c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(text):\n",
    "    completion = client.chat.completions.create(\n",
    "      # model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        model=\"gec-llama2-7b-public/\",\n",
    "        temperature=1,\n",
    "        n=5,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Rewrite this text to make it grammatically correct .\"},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ]\n",
    "    )\n",
    "    preds = []\n",
    "    for choise in completion.choices:\n",
    "        pred = choise.message.content\n",
    "        prefixes = [\"text:\", \"text is:\", \":\\n\\n\", \":\\n\"]\n",
    "        for p in prefixes:\n",
    "            if p in pred:\n",
    "                pred = pred[pred.index(p)+len(p)+1:]\n",
    "                pred = pred.lstrip()\n",
    "                if \"\\n\" in pred:\n",
    "                    pred = pred[:pred.index('\\n')]\n",
    "                break\n",
    "        preds.append(pred)\n",
    "    return preds\n",
    "\n",
    "infer(\"I hop ths mesage find u ..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bcedb3-b045-42be-ad39-4ccc2660bc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['pred-llama2-sample'] = df_sample.src.progress_map(infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287c143a-1990-4836-9125-6ad717707b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.to_json('troy-llama7-gec-20k-sample.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539d41ab-971a-48c8-b635-336d7013557c",
   "metadata": {},
   "source": [
    "# Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17843760-8063-4d7d-9e0c-2b83690026ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['score'] = df_sample.apply(lambda x: scorer.score(x.src, x['pred-llama2-sample']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f848f5-5aec-4198-b602-04c70aa63b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dpo = []\n",
    "\n",
    "for i, row in df_samples.iterrows():\n",
    "    scores = results[i]\n",
    "    max_score = max(list( map(lambda x: x['score'], scores.values())))\n",
    "    max_model = list(filter(lambda x: x[1]['score']==max_score, scores.items()))[0][0]\n",
    "\n",
    "    min_score = min(list( map(lambda x: x['score'], scores.values())))\n",
    "    min_model = list(filter(lambda x: x[1]['score']==min_score, scores.items()))[0][0]\n",
    "    data_dpo.append({\n",
    "        'src': row.src,\n",
    "        'choosen': row[max_model],\n",
    "        'rejected': row[min_model]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb0c950-a375-4966-83f4-e0f3f62d9ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data_dpo).to_json('troy-llama7-gec-20k-sample-scribendi.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
