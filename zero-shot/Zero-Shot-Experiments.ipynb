{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59fb16ee-5c46-49ee-8e1e-aacddb2772f6",
   "metadata": {},
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "raw",
   "id": "73f07181-2c40-4f9d-9dbb-47c680f883fb",
   "metadata": {},
   "source": [
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U transformers==4.38.2\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q -U trl xformers wandb datasets einops gradio sentencepiece bitsandbytes nltk spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aee83a-35ab-42be-8ad4-c6de808481ed",
   "metadata": {},
   "source": [
    "# LLaMA2 evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f9df37-0fd9-40a8-8b55-91473bc02992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['WANDB_MODE']='disabled'\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "import platform\n",
    "import gradio\n",
    "import warnings\n",
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline, logging, TextStreamer, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, DPOTrainer, KTOConfig, KTOTrainer\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import spacy\n",
    "NLP = spacy.load('en_core_web_sm')\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokenize(txt):\n",
    "    doc = NLP(txt)\n",
    "    return \" \".join([t.text for t in doc])\n",
    "\n",
    "import subprocess\n",
    "def run(cmd):\n",
    "    print(\"Run shell command:\\n\\t\", cmd)\n",
    "    return subprocess.run(cmd, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e278b7d4-f647-43d6-9a4f-bda754911a3a",
   "metadata": {},
   "source": [
    "# Define Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c404a8d7-05d6-4a04-9da2-c161b2e3a39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTIONS = [\n",
    "    \"Fix grammatical errors for the following text . Keep only one variant .\",\n",
    "    \"Rewrite this text to make it grammatically correct . \",\n",
    "    \"Rewrite the text to fix any grammatical errors .\",\n",
    "    \"Correct the grammar mistakes in the following text .\",\n",
    "    \"Rewrite the text . The output text should not contain any grammatical or spelling mistakes .\",\n",
    "    \"Fix all grammatical errors , do not rephrase .\",\n",
    "    \"Fix only grammatical errors precisely. \",\n",
    "    \"Precisely fix grammatical errors : \",\n",
    "\n",
    "    \"Revise the following sentence with proper grammar\",\n",
    "    \"Correct grammatical errors in this sentence .\",\n",
    "    \"Revise grammatical mistakes in the following text.\",\n",
    "    \"Revise mistakes in the following text written by a beginner learner with a lot of mistakes.\",\n",
    "    \"Revise mistakes in the following text written by a advanced learner with a few of mistakes.\",\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d48713-56d4-4d89-9fcc-df92dd6b5a49",
   "metadata": {},
   "source": [
    "# Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d92b51f-e0e1-4365-ae70-40bb4a432b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# Setup quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant= False,\n",
    ")\n",
    "\n",
    "# Load model and setup quantization\n",
    "use_flash_attn = False\n",
    "load_in_8bit = False\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    "    load_in_8bit=load_in_8bit,\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\" if use_flash_attn else \"eager\"\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.config.use_cache = False # silence the warnings. Please re-enable for inference!\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c95bec-b89d-445b-b0ad-6c5b57b10c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.add_bos_token, tokenizer.add_eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dba81b5-681f-4be9-84a0-f287e7c6bd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gec_predict(model, tokenizer, text, prompt_id=0):\n",
    "    answers = [\n",
    "        \"Sure, here's the corrected text:\",\n",
    "        \"Sure, here is a rewritten version of the text that is grammatically correct:\",\n",
    "        \"Here is a corrected version of the text:\",\n",
    "        \"Here's a corrected version of the sentence:\",\n",
    "        \"Here is the rewritten text:\",\n",
    "        \"Here is the corrected text:\",\n",
    "        \"Here is a revised version of the text that is grammatically correct:\",\n",
    "        \"Sure! Here's the corrected text:\",\n",
    "        \"Here is a revised version of the text that is grammatically correct:\",\n",
    "        \"Here is a rewritten version of the text that is grammatically correct:\",\n",
    "        \"Sure, here's a corrected version of the sentence:\",\n",
    "        \"Sure, here's a corrected version of the text:\",\n",
    "        \"Here is the rewritten text with corrected grammar:\",\n",
    "        \"Here is a corrected version of the sentence:\",\n",
    "        \"Sure, here is the corrected text:\",\n",
    "        \"Sure, here is a corrected version of the sentence:\",\n",
    "        \"Sure, here is the rewritten text:\",\n",
    "        \"Sure, I'd be happy to help! Here is a revised version of the sentence that is grammatically correct:\",\n",
    "        \"Sure, here's the rewritten text:\",\n",
    "        \"Here is a rewritten version of the text with corrected grammar and spelling:\",\n",
    "        \"Here is the rewritten text with corrected grammar and punctuation:\",\n",
    "        \"Here is a rewritten version of the text with grammatical corrections:\",\n",
    "        \"Sure, here is a corrected version of the text:\"\n",
    "    ]\n",
    "    \n",
    "    instruction = INSTRUCTIONS[prompt_id]\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a writing assistant. Please ensure that your responses consist only of corrected texts.\"},\n",
    "        # {\"role\": \"user\", \"content\": \"Fix grammatical errors for the following text: \\\"Ths text contains to email best ..\\\"\"},\n",
    "        # {\"role\": \"assistant\", \"content\": \"This text contains the best email .\"},\n",
    "        {\"role\": \"user\", \"content\": instruction + f\"\\\"{text}\\\"\"},\n",
    "    ]\n",
    "    \n",
    "    input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "    generated_ids = model.generate(input_ids, max_new_tokens=1000, do_sample=False, temperature=0.001)\n",
    "    output = tokenizer.batch_decode(generated_ids)[0]\n",
    "\n",
    "    _output = output[output.index('[/INST]') + len('[/INST]') :]\n",
    "\n",
    "    if \":\\n\" in _output:\n",
    "        _output = _output[_output.index(\":\\n\"):]\n",
    "    \n",
    "    _output = list(filter(lambda x: len(x)>1 and x not in answers, _output.split('\\n')))[0]\n",
    "    _output = _output.replace(\"</s>\", \"\").replace('\\n', '').replace(\"\\\"\", '')\n",
    "\n",
    "    return _output \n",
    "\n",
    "\n",
    "\n",
    "gec_predict(model, tokenizer, \"Ths text contains to email best .\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa42975-5f80-4f2d-934d-c43443cee554",
   "metadata": {},
   "source": [
    "## Evaluate NUCLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabe05c2-5b29-4e30-85d9-1e0a0600c4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range( 0, 13):\n",
    "    df = pd.read_csv('nucle.test.csv')\n",
    "    df['output'] = df.src.progress_map(lambda x: gec_predict(model, tokenizer, x, i))\n",
    "    df['output'] = df.output.map(tokenize)\n",
    "    df.to_csv(f'outputs/nucle-llama-7b-chat-{i}.csv', index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e74c50-d279-4d02-b854-269d9b013190",
   "metadata": {},
   "source": [
    "## Evaluate BEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b74e2c-7704-4d1d-9357-df3894addd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEA\n",
    "cmd = \"\"\"\n",
    "docker run -it --rm  -v bea-dev.txt:/data/input.txt \\\n",
    "    -v bea-dev.m2:/data/ref.m2 \\\n",
    "    -v {pred_txt}:/data/pred.txt \\\n",
    "    errant \\\n",
    "        python3 /errant/parallel_to_m2.py -orig /data/input.txt -cor /data/pred.txt -out /data/pred.m2 && \\\n",
    "        python3 /errant/compare_m2.py -hyp /data/pred.m2 -ref /data/ref.m2\n",
    "\"\"\"\n",
    "\n",
    "for i in range( 0, 13):\n",
    "    df = pd.read_csv('wi_locness.dev.gold.bea18.csv')\n",
    "    df['output'] = df.src.progress_map(lambda x: gec_predict(model, tokenizer, x, i))\n",
    "    df['output'] = df.output.map(tokenize)\n",
    "    df.to_csv(f'outputs/bea-llama-7b-chat-{i}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbe9acb-873f-4409-97cb-09e87a7e5ab8",
   "metadata": {},
   "source": [
    "# Evaluate Mistral AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf1acb6-0f12-4967-8e66-1afc97631410",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Setup quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant= True,\n",
    "    llm_int8_enable_fp32_cpu_offload= True\n",
    ")\n",
    "\n",
    "# Load model and setup quantization\n",
    "use_flash_attn = False\n",
    "load_in_8bit = False\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    "    load_in_8bit=load_in_8bit,\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\" if use_flash_attn else \"eager\"\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.config.use_cache = False # silence the warnings. Please re-enable for inference!\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651eec28-a20f-41d9-b1a2-cd9ee5df12a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gec_predict(model, tokenizer, text, prompt_id=0):\n",
    "    answers = [\n",
    "        \"Sure, here's the corrected text:\",\n",
    "        \"Sure, here is a rewritten version of the text that is grammatically correct:\",\n",
    "        \"Here is a corrected version of the text:\",\n",
    "        \"Here's a corrected version of the sentence:\",\n",
    "        \"Here is the rewritten text:\",\n",
    "        \"Here is the corrected text:\",\n",
    "        \"Here is a revised version of the text that is grammatically correct:\",\n",
    "        \"Sure! Here's the corrected text:\",\n",
    "        \"Here is a revised version of the text that is grammatically correct:\",\n",
    "        \"Here is a rewritten version of the text that is grammatically correct:\",\n",
    "        \"Sure, here's a corrected version of the sentence:\",\n",
    "        \"Sure, here's a corrected version of the text:\",\n",
    "        \"Here is the rewritten text with corrected grammar:\",\n",
    "        \"Here is a corrected version of the sentence:\",\n",
    "        \"Sure, here is the corrected text:\",\n",
    "        \"Sure, here is a corrected version of the sentence:\",\n",
    "        \"Sure, here is the rewritten text:\",\n",
    "        \"Sure, I'd be happy to help! Here is a revised version of the sentence that is grammatically correct:\",\n",
    "        \"Sure, here's the rewritten text:\",\n",
    "        \"Here is a rewritten version of the text with corrected grammar and spelling:\",\n",
    "        \"Here is the rewritten text with corrected grammar and punctuation:\",\n",
    "        \"Here is a rewritten version of the text with grammatical corrections:\",\n",
    "        \"Sure, here is a corrected version of the text:\"\n",
    "    ]\n",
    "    \n",
    "    instruction = INSTRUCTIONS[prompt_id]\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a writing assistant. Please ensure that your responses consist only of corrected texts.\"},\n",
    "        {\"role\": \"user\", \"content\": instruction + f\"\\\"{text}\\\"\"},\n",
    "    ]\n",
    "    \n",
    "    input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "    generated_ids = model.generate(input_ids, max_new_tokens=1000, do_sample=False, temperature=0.001)\n",
    "    output = tokenizer.batch_decode(generated_ids)[0]\n",
    "    \n",
    "    _output = output[output.index('[/INST]') + len('[/INST]') :]\n",
    "    if \"grammatically correct\" in output:\n",
    "        return text\n",
    "\n",
    "    if \":\\n\" in _output:\n",
    "        _output = _output[_output.index(\":\\n\"):]\n",
    "    \n",
    "    _output = list(filter(lambda x: len(x)>1 and x not in answers, _output.split('\\n')))[0]\n",
    "    _output = _output.replace(\"</s>\", \"\").replace('\\n', '').replace(\"\\\"\", '')\n",
    "\n",
    "    _output = sent_tokenize(_output)[0].strip()\n",
    "    return _output \n",
    "\n",
    "gec_predict(model, tokenizer, \"Ths text contains to email best .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c281d987-5d3e-4fb6-825f-3c45bb7eb5dd",
   "metadata": {},
   "source": [
    "## Evaluate NUCLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8970fc-e7ed-47c6-b68c-8bf399840a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range( 0, 13):\n",
    "    df = pd.read_csv('nucle.test.csv')\n",
    "    df['output'] = df.src.progress_map(lambda x: gec_predict(model, tokenizer, x, i))\n",
    "    df['output'] = df.output.map(tokenize)\n",
    "    df.to_csv(f'outputs/nucle-mistral-{i}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26060975-25ea-408b-98cd-3df432e4d3b2",
   "metadata": {},
   "source": [
    "## Evaluate BEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184844dd-f754-4b91-8767-978b53c33d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = \"\"\"\n",
    "docker run -it --rm  -v /mnt/green-efs/kostiantyn.omelianchuk/gec_sota/data/evalsets/bea-dev.txt:/data/input.txt \\\n",
    "    -v /mnt/green-efs/kostiantyn.omelianchuk/gec_sota/data/evalsets/bea-dev.m2:/data/ref.m2 \\\n",
    "    -v {pred_txt}:/data/pred.txt \\\n",
    "    errant \\\n",
    "        python3 /errant/parallel_to_m2.py -orig /data/input.txt -cor /data/pred.txt -out /data/pred.m2 && \\\n",
    "        python3 /errant/compare_m2.py -hyp /data/pred.m2 -ref /data/ref.m2\n",
    "\"\"\"\n",
    "\n",
    "for i in [6]:\n",
    "    df = pd.read_csv('/mnt/green-efs/oleksandr.korniienko/data/wi_locness.dev.gold.bea18.csv')\n",
    "    df['output'] = df.src.progress_map(lambda x: gec_predict(model, tokenizer, x, i))\n",
    "    df['output'] = df.output.map(tokenize)\n",
    "    df.to_csv(f'outputs/bea-mistral-{i}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc261e3-e0f9-4c41-8df7-adbd8c619c89",
   "metadata": {},
   "source": [
    "# Evaluate GEMMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938c3d58-4418-4583-85cd-91a4b2b83423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"google/gemma-2b-it\"\n",
    "model_id = \"google/gemma-1.1-7b-it\"\n",
    "model_id = \"google/gemma-1.1-2b-it\"\n",
    "\n",
    "\n",
    "# Setup quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant= False,\n",
    ")\n",
    "\n",
    "# Load model and setup quantization\n",
    "use_flash_attn = False\n",
    "load_in_8bit = False\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    "    load_in_8bit=load_in_8bit,\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\" if use_flash_attn else \"eager\"\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.config.use_cache = False # silence the warnings. Please re-enable for inference!\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669064b5-73d5-482a-a710-892b31483d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gec_predict(model, tokenizer, text):\n",
    "    answers = [\n",
    "        \"Sure, here's the corrected text:\",\n",
    "        \"Sure, here is a rewritten version of the text that is grammatically correct:\",\n",
    "        \"Here is a corrected version of the text:\",\n",
    "        \"Here's a corrected version of the sentence:\",\n",
    "        \"Here is the rewritten text:\",\n",
    "        \"Here is the corrected text:\",\n",
    "        \"Here is a revised version of the text that is grammatically correct:\",\n",
    "        \"Sure! Here's the corrected text:\",\n",
    "        \"Here is a revised version of the text that is grammatically correct:\",\n",
    "        \"Here is a rewritten version of the text that is grammatically correct:\",\n",
    "        \"Sure, here's a corrected version of the sentence:\",\n",
    "        \"Sure, here's a corrected version of the text:\",\n",
    "        \"Here is the rewritten text with corrected grammar:\",\n",
    "        \"Here is a corrected version of the sentence:\",\n",
    "        \"Sure, here is the corrected text:\",\n",
    "        \"Sure, here is a corrected version of the sentence:\",\n",
    "        \"Sure, here is the rewritten text:\",\n",
    "        \"Sure, I'd be happy to help! Here is a revised version of the sentence that is grammatically correct:\",\n",
    "        \"Sure, here's the rewritten text:\",\n",
    "        \"Here is a rewritten version of the text with corrected grammar and spelling:\",\n",
    "        \"Here is the rewritten text with corrected grammar and punctuation:\",\n",
    "        \"Here is a rewritten version of the text with grammatical corrections:\",\n",
    "        \"Sure, here is a corrected version of the text:\"\n",
    "    ]\n",
    "    \n",
    "    instruction = INSTRUCTIONS[7]\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a writing assistant. Please ensure that your responses consist only of corrected texts.\"},\n",
    "        # {\"role\": \"user\", \"content\": \"Fix grammatical errors for the following text: \\\"Ths text contains to email best ..\\\"\"},\n",
    "        # {\"role\": \"assistant\", \"content\": \"This text contains the best email .\"},\n",
    "        {\"role\": \"user\", \"content\": instruction + f\"\\\"{text}\\\"\"},\n",
    "    ]\n",
    "    \n",
    "    input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "    generated_ids = model.generate(input_ids, max_new_tokens=1000, do_sample=False, temperature=0.001)\n",
    "    output = tokenizer.batch_decode(generated_ids)[0]\n",
    "    \n",
    "    _output = output[output.rfind('\\n'):]\n",
    "    if '\"' in _output:\n",
    "        _output = _output[_output.find('\"'):_output.rfind('\"')]\n",
    "    _output = _output.replace('\\n', '').replace(\"\\\"\", '').replace(\"<eos>\", \"\")\n",
    "    return _output \n",
    "\n",
    "\n",
    "\n",
    "# gec_predict(model, tokenizer, \"Ths text contains to email best .\")\n",
    "gec_predict(model, tokenizer, \"Keeping the Secret of Genetic Testing\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b3abd3-b060-4722-98c7-7ce17834295f",
   "metadata": {},
   "source": [
    "## Evaluate NUCLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec855bf-3a80-4842-b410-1c08a0a21d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range( 0, 13):\n",
    "    df = pd.read_csv('nucle.test.csv')\n",
    "    df['output'] = df.src.progress_map(lambda x: gec_predict(model, tokenizer, x, i))\n",
    "    df['output'] = df.output.map(tokenize)\n",
    "    df.to_csv(f'outputs/nucle-gemma-2b-{i}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204c70b1-9e29-4247-870a-a1bd1669afdc",
   "metadata": {},
   "source": [
    "## Evaluate BEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1eabe8-51fc-4b5d-9402-aeee0c9d037b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = \"\"\"\n",
    "docker run -it --rm  -v /mnt/green-efs/kostiantyn.omelianchuk/gec_sota/data/evalsets/bea-dev.txt:/data/input.txt \\\n",
    "    -v /mnt/green-efs/kostiantyn.omelianchuk/gec_sota/data/evalsets/bea-dev.m2:/data/ref.m2 \\\n",
    "    -v {pred_txt}:/data/pred.txt \\\n",
    "    errant \\\n",
    "        python3 /errant/parallel_to_m2.py -orig /data/input.txt -cor /data/pred.txt -out /data/pred.m2 && \\\n",
    "        python3 /errant/compare_m2.py -hyp /data/pred.m2 -ref /data/ref.m2\n",
    "\"\"\"\n",
    "\n",
    "for i in [6]:\n",
    "    df = pd.read_csv('/mnt/green-efs/oleksandr.korniienko/data/wi_locness.dev.gold.bea18.csv')\n",
    "    df['output'] = df.src.progress_map(lambda x: gec_predict(model, tokenizer, x, i))\n",
    "    df['output'] = df.output.map(tokenize)\n",
    "    df.to_csv(f'outputs/bea-mistral-{i}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79de087a-38a6-473a-8c2e-f9b7ffcc1999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2307b2bb-b506-46de-9b7d-3107e57da3e3",
   "metadata": {},
   "source": [
    "# Start server with model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b9bc442-f31c-431d-8d8e-ca7205778003",
   "metadata": {},
   "source": [
    "pip install vllm\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=1,2,3 python -m vllm.entrypoints.openai.api_server --model google/gemma-1.1-7b-it --dtype auto --api-key token-abc123 --max-model-len 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6fabd6-40f6-4533-88f0-cdaf1292058c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run inference with openai-like endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e3aeb1-878a-426f-99fa-f7279eb5dfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"token-abc123\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89bd920-5206-40fd-a757-c70179f0915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2074d5a-1400-4747-90e8-ef54960204c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/nucle.test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d34307b-9848-421d-a31d-704226609e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(text):\n",
    "    completion = client.chat.completions.create(\n",
    "      # model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        model=\"gec-llama2-7b-public/\",\n",
    "        temperature=1,\n",
    "        n=5,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Rewrite this text to make it grammatically correct .\"},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ]\n",
    "    )\n",
    "    preds = []\n",
    "    for choise in completion.choices:\n",
    "        pred = choise.message.content\n",
    "        prefixes = [\"text:\", \"text is:\", \":\\n\\n\", \":\\n\"]\n",
    "        for p in prefixes:\n",
    "            if p in pred:\n",
    "                pred = pred[pred.index(p)+len(p)+1:]\n",
    "                pred = pred.lstrip()\n",
    "                if \"\\n\" in pred:\n",
    "                    pred = pred[:pred.index('\\n')]\n",
    "                break\n",
    "        preds.append(pred)\n",
    "    return preds\n",
    "\n",
    "infer(\"I hop ths mesage find u ..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfebd0d-a41f-4f61-b25e-090d3bcac030",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['output'] = df.src.progress_map(infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17319f32-b0a1-4341-92af-a14dc17d3e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/nucle.test.output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
