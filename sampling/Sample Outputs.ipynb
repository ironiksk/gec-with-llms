{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33ef53c2-4eb8-4646-9510-4047bb03bb48",
   "metadata": {},
   "source": [
    "# Sample outputs with LLaMA-Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d0acb0-a534-489e-9cdf-51b33ff263b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8e9ccd-02a0-4b44-815e-afe62b9b4ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import spacy\n",
    "NLP = spacy.load('en_core_web_sm')\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokenize(txt):\n",
    "    doc = NLP(txt)\n",
    "    return \" \".join([t.text for t in doc])\n",
    "\n",
    "import subprocess\n",
    "def run(cmd):\n",
    "    print(\"Run shell command:\\n\\t\", cmd)\n",
    "    return subprocess.run(cmd, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ceb263-1553-4f15-a6dd-ff3c51887d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_PATH = \"./LLaMA-Factory\"\n",
    "\n",
    "checkpoint = '2800'\n",
    "\n",
    "# llama 6k updates\n",
    "model_name = \"LLaMA2-7B-chat-gec\"\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    checkpoint = ['200', '2800', '800', '1200', '2000'][i]\n",
    "    model_path = f\"{PROJECT_PATH}/saves/{model_name}/lora-all-6k/sft/checkpoint-{checkpoint}\"\n",
    "\n",
    "    \n",
    "    print(model_path)\n",
    "    \n",
    "    data_path = f'{model_path}/predict-nucle-samples/generated_predictions.jsonl'\n",
    "    output_path = f'{model_path}/predict-nucle-samples/output-{i}.csv'\n",
    "    \n",
    "    cmd_infer = f\"\"\"\n",
    "    WANDB_DISABLED=true CUDA_VISIBLE_DEVICES=0,1,2,3 accelerate launch \\\n",
    "        --config_file {PROJECT_PATH}/examples/accelerate/single_config.yaml \\\n",
    "        {PROJECT_PATH}/src/train_bash.py \\\n",
    "        --stage sft \\\n",
    "        --do_predict \\\n",
    "        --model_name_or_path {model_id} \\\n",
    "        --adapter_name_or_path {model_path} \\\n",
    "        --dataset gec_dataset_test \\\n",
    "        --dataset_dir {PROJECT_PATH}/data \\\n",
    "        --template default \\\n",
    "        --temperature 1.0 \\\n",
    "        --do_sample true \\\n",
    "        --finetuning_type lora \\\n",
    "        --lora_target q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj \\\n",
    "        --output_dir {model_path}/predict-nucle-samples \\\n",
    "        --overwrite_cache \\\n",
    "        --overwrite_output_dir \\\n",
    "        --cutoff_len 256 \\\n",
    "        --preprocessing_num_workers 16 \\\n",
    "        --per_device_eval_batch_size 1 \\\n",
    "        --max_samples 1311 \\\n",
    "        --predict_with_generate \\\n",
    "        --fp16\n",
    "    \"\"\"\n",
    "    \n",
    "    run(cmd_infer)\n",
    "\n",
    "    \n",
    "    with open(data_path, 'r') as f:\n",
    "        data = [json.loads(l) for l in f.readlines()]\n",
    "    \n",
    "    df_output = pd.DataFrame(data)\n",
    "    df = pd.read_csv('./data/nucle.test.csv')\n",
    "    df['output'] = df_output['predict']\n",
    "    df['output'] = df.output.map(tokenize)\n",
    "    df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8337304a-2c0b-4657-8b77-5175cae22ad3",
   "metadata": {},
   "source": [
    "# Sample outputs with OpenAI-like interface"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dfe83fb5-ac6a-4ef5-8d25-393a05191740",
   "metadata": {},
   "source": [
    "pip install vllm\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=1,2,3 python -m vllm.entrypoints.openai.api_server --model google/gemma-1.1-7b-it --dtype auto --api-key token-abc123 --max-model-len 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2995a4-f208-4416-82ef-f9c0e82277c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"token-abc123\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc45d34-80f8-4e29-8fed-e378e154816a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f659ae-25a1-4f1e-abc5-831bd18aafc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(text):\n",
    "    completion = client.chat.completions.create(\n",
    "      # model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        model=\"gec-llama2-7b-public/\",\n",
    "        temperature=1,\n",
    "        n=5,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Rewrite this text to make it grammatically correct .\"},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ]\n",
    "    )\n",
    "    preds = []\n",
    "    for choise in completion.choices:\n",
    "        pred = choise.message.content\n",
    "        prefixes = [\"text:\", \"text is:\", \":\\n\\n\", \":\\n\"]\n",
    "        for p in prefixes:\n",
    "            if p in pred:\n",
    "                pred = pred[pred.index(p)+len(p)+1:]\n",
    "                pred = pred.lstrip()\n",
    "                if \"\\n\" in pred:\n",
    "                    pred = pred[:pred.index('\\n')]\n",
    "                break\n",
    "        preds.append(pred)\n",
    "    return preds\n",
    "\n",
    "infer(\"I hop ths mesage find u ..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e9abed-3f46-4f9c-8e75-9e4748bb19a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('troy-blogs.train.tokenized.csv')\n",
    "df_sample = df.sample(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2acd081-f4bd-4aa4-bcf3-475a23f1858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['outputs'] = df_sample.src.progress_map(infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dcb5a4-4b09-437e-bc68-9b5e8eee4c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.to_csv('troy-blogs.train.tokenized-llama2-samples.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
