{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f5ccde-0e11-4dea-ac02-3bec2b5646d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model\n",
    "\n",
    "pip install vllm\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=1,2,3 python -m vllm.entrypoints.openai.api_server --model google/gemma-1.1-7b-it --dtype auto --api-key token-abc123 --max-model-len 2048\n",
    "\n",
    "\n",
    "/red-phantasm-efs/rewrites/llama2-gec-sota/llama-2-7b-chat-hf-batch-4-acc-2-lr-1e-05-up-800-wup-100-seed-9453-gec_sota-gec_sota-bea-all/models\n",
    "/red-phantasm-efs/rewrites/llama2-gec-sota/llama-2-13b-hf-batch-4-acc-2-lr-1e-05-up-800-wup-100-seed-9453-gec_sota-gec_sota-bea-all/models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64acce51-3043-47e1-a8b3-9b1e2966b851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aa2f4d-d7fd-43cf-988b-64c6af72cf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102ccb60-8272-4712-957d-089ebf85fb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('troy-blogs.train.tokenized.csv')\n",
    "df_sample = df.sample(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892cd066-add4-4384-b144-1d9a1ad39959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"token-abc123\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0b36a2-f175-44b4-9d3e-d92659335af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(text):\n",
    "    completion = client.chat.completions.create(\n",
    "      # model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        model=\"gec-llama2-7b-public/\",\n",
    "        temperature=1,\n",
    "        n=5,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Rewrite this text to make it grammatically correct .\"},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ]\n",
    "    )\n",
    "    preds = []\n",
    "    for choise in completion.choices:\n",
    "        pred = choise.message.content\n",
    "        prefixes = [\"text:\", \"text is:\", \":\\n\\n\", \":\\n\"]\n",
    "        for p in prefixes:\n",
    "            if p in pred:\n",
    "                pred = pred[pred.index(p)+len(p)+1:]\n",
    "                pred = pred.lstrip()\n",
    "                if \"\\n\" in pred:\n",
    "                    pred = pred[:pred.index('\\n')]\n",
    "                break\n",
    "        preds.append(pred)\n",
    "    return preds\n",
    "\n",
    "infer(\"I hop ths mesage find u ..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637f327c-8209-402c-b342-30e366f016ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['pred-llama2-sample'] = df_sample.src.progress_map(infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b555537-6a60-4c5e-bb13-c3b643f4a08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.to_json('troy-llama7-gec-20k-sample.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575d74c3-7664-41b1-82b7-0d8cf4f20022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00baa19a-d2a5-44c8-b3f7-8c932087189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score samples with Greco / Scibelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d388a5-9228-4a9d-9cd4-836b1ea15cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from greco_model import GRECO\n",
    "\n",
    "\n",
    "def get_top_candidate(source, hyps, sys_names, model, voting_bias=True, m7_bias=0, verbose=0):\n",
    "    if len(hyps) == 1:\n",
    "        return hyps[0], sys_names[0]\n",
    "\n",
    "    \n",
    "    # score hypothesis\n",
    "    scores = model.score(source, hyps)\n",
    "    scores = scores.tolist()\n",
    "    \n",
    "    # get sys_weights based on majority voting\n",
    "    # sys_proportions = [len(x.split(\"|\")) for x in sys_names]\n",
    "    # sys_proportions = [len(x.split(\"|\")) + (1 if \"orig\" in x else 0) for x in sys_names]\n",
    "    sys_proportions = [len(x.split(\"|\")) + (m7_bias if \"ens_m\" in x else 0) for x in sys_names]\n",
    "    # TODO: find out weighting factor\n",
    "    wf = sum(sys_proportions)\n",
    "    wn = wf/len(sys_proportions)\n",
    "    mn = max(sys_proportions)\n",
    "    # voting_bias = 1\n",
    "    # sys_proportions = [(x )/wf for x in sys_proportions]\n",
    "    # sys_proportions = [np.log(x + 1) for x in sys_proportions]\n",
    "    # sys_proportions = [(x)/wn for x in sys_proportions]\n",
    "    sys_proportions = [(x)/mn for x in sys_proportions]\n",
    "    \n",
    "    if voting_bias is not None:\n",
    "        scores = [s * sp for s, sp in zip(scores,sys_proportions)]\n",
    "    # scores = [sp * sp for s, sp in zip(scores,sys_proportions)]\n",
    "\n",
    "    sorted_hyps = sorted([(s, n, hyp) for hyp, n, s in zip(hyps, sys_names, scores)], reverse=True)\n",
    "    if verbose > 0:\n",
    "        print(f\"\\nOriginal: {source}\")\n",
    "        for i, (s, name, hyp) in enumerate(sorted_hyps):\n",
    "            print(f\"Top {i+ 1} hyp is {name} with {s} score: {hyp}\")\n",
    "        print('\\n')\n",
    "    top_hyp, top_sys = sorted_hyps[0][-1], sorted_hyps[0][-2]  \n",
    "    return top_hyp, top_sys\n",
    "\n",
    "\n",
    "def get_choosen_rejected(source, hyps, model, voting_bias=True, m7_bias=0, verbose=0):\n",
    "    if len(hyps) == 1:\n",
    "        return hyps[0], hyps[0]\n",
    "    \n",
    "    # score hypothesis\n",
    "    scores = model.score(source, hyps)\n",
    "    scores = scores.tolist()\n",
    "\n",
    "    pi = max(enumerate(scores), key=lambda x:x[1])[0]\n",
    "    ni = min(enumerate(scores), key=lambda x:x[1])[0]\n",
    "\n",
    "    return hyps[pi], hyps[ni]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dc5515-6fa3-446b-8d73-9c91fc9f7355",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "path_to_model = \"/mnt/green-efs/kostiantyn.omelianchuk/gec_sota/models/greco/models/checkpoint.bin\"\n",
    "model = GRECO('microsoft/deberta-v3-large').to(device)\n",
    "# model.load_state_dict(torch.load(path_to_model))\n",
    "weights = torch.load(path_to_model)\n",
    "model.load_state_dict({n:weights[n] for n, p in model.named_parameters() })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b833c43-b1c9-43df-bc31-8d78591eed48",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"He go at school .\"\n",
    "hyps = [\"He goes to school .\", \"He goes at school .\", \"He go at school .\", \"He go .\", ]\n",
    "# sys_names = ['best', 'second|third', 'orig', 'broken']\n",
    "# get_top_candidate(source, hyps, sys_names, model, verbose=1)\n",
    "\n",
    "get_choosen_rejected(source, hyps, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df1a26f-30ff-4a60-b7be-3fa9cb6a7971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1662d1e5-b05e-417f-8f94-f2567156ae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('troy-llama7-gec-20k-sample.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02124a6-a43f-4b96-af1b-c738884b1460",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = []\n",
    "positives = []\n",
    "negatives = []\n",
    "samples = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows()):\n",
    "    # s = detokenize(row.src.split(\" \"))\n",
    "    s = row.src\n",
    "\n",
    "    p, n = get_choosen_rejected(s, row['pred-llama2-sample'], model)\n",
    "    \n",
    "    positives.append(p)\n",
    "    negatives.append(n)\n",
    "    src.append(s)\n",
    "\n",
    "    d = {\n",
    "        \"question\": row.src, \n",
    "        \"answer\": [\n",
    "            p.strip(),\n",
    "            n.strip()\n",
    "        ], \n",
    "        \"system\": \"Rewrite this text to make it grammatically correct .\"\n",
    "    } \n",
    "    samples.append(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd710750-9288-4971-bef0-04fe4f579858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# pd.DataFrame(samples).to_json('llama7_gec_greco.json')\n",
    "with open('llama7_gec_greco.json', \"w\") as f:\n",
    "    json.dump(samples, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d415f06-e586-400e-a60b-e26e0928dfc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60358318-4c9f-4f24-bc3e-7faeac66132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune LLama with DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ca7d01-d2a1-4f02-9882-9fef9da73a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "git clone https://github.com/hiyouga/LLaMA-Factory.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9116b1c4-974b-43c4-84b5-28c2f3e4f0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_DISABLED=true CUDA_VISIBLE_DEVICES=0,1,2,3 accelerate launch \\\n",
    "    --config_file ../accelerate/single_config.yaml \\\n",
    "    ../../src/train_bash.py \\\n",
    "    --stage dpo \\\n",
    "    --do_train \\\n",
    "    --model_name_or_path /home/oleksandr.korniienko/gec-llama2-7b-public/ \\\n",
    "    --create_new_adapter \\\n",
    "    --dataset llama7_gec_greco \\\n",
    "    --dataset_dir ../../data \\\n",
    "    --template llama2 \\\n",
    "    --finetuning_type lora \\\n",
    "    --lora_target q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj \\\n",
    "    --output_dir ../../saves/LLaMA2-7B-gec-greco/dpo-02/ \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 512 \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 50 \\\n",
    "    --warmup_steps 20 \\\n",
    "    --save_steps 200 \\\n",
    "    --eval_steps 200 \\\n",
    "    --evaluation_strategy steps \\\n",
    "    --load_best_model_at_end \\\n",
    "    --learning_rate 1e-5 \\\n",
    "    --num_train_epochs 2.0 \\\n",
    "    --val_size 0.1 \\\n",
    "    --dpo_ftx 1.0 \\\n",
    "    --plot_loss \\\n",
    "    --fp16 \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --ddp_find_unused_parameters False \\\n",
    "    --dpo_beta 0.2\n",
    "\n",
    "\n",
    "WANDB_DISABLED=true CUDA_VISIBLE_DEVICES=0,1,2,3 accelerate launch \\\n",
    "    --config_file ../accelerate/single_config.yaml \\\n",
    "    ../../src/train_bash.py \\\n",
    "    --stage sft \\\n",
    "    --do_predict \\\n",
    "    --model_name_or_path /home/oleksandr.korniienko/gec-llama2-7b-public/ \\\n",
    "    --adapter_name_or_path ../../saves/LLaMA2-7B-gec-greco/dpo-02/checkpoint-400/ \\\n",
    "    --dataset gec_nucle_test \\\n",
    "    --dataset_dir ../../data \\\n",
    "    --template llama2 \\\n",
    "    --finetuning_type lora \\\n",
    "    --lora_target q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj \\\n",
    "    --output_dir ../../saves/LLaMA2-7B-gec-greco/dpo-02/predict-nucle/ \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 256 \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --max_samples 1311 \\\n",
    "    --fp16 \\\n",
    "    --predict_with_generate\n",
    "    \n",
    "\n",
    "WANDB_DISABLED=true CUDA_VISIBLE_DEVICES=0,1,2,3 accelerate launch \\\n",
    "    --config_file ../accelerate/single_config.yaml \\\n",
    "    ../../src/train_bash.py \\\n",
    "    --stage sft \\\n",
    "    --do_predict \\\n",
    "    --model_name_or_path /home/oleksandr.korniienko/gec-llama2-7b-public/ \\\n",
    "    --adapter_name_or_path ../../saves/LLaMA2-7B-gec-greco/dpo-02/checkpoint-400/ \\\n",
    "    --dataset gec_bea_dev \\\n",
    "    --dataset_dir ../../data \\\n",
    "    --template llama2 \\\n",
    "    --finetuning_type lora \\\n",
    "    --lora_target q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj \\\n",
    "    --output_dir ../../saves/LLaMA2-7B-gec-greco/dpo-02/predict-bea/ \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 256 \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --max_samples 4384 \\\n",
    "    --predict_with_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39638b23-9c89-418f-838d-3755859fe098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd264224-5014-4894-a89a-909af93afafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8216d260-284d-4cd3-bcf8-dd42a325c725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import spacy\n",
    "NLP = spacy.load('en_core_web_sm')\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokenize(txt):\n",
    "    doc = NLP(txt)\n",
    "    return \" \".join([t.text for t in doc])\n",
    "\n",
    "import subprocess\n",
    "def run(cmd):\n",
    "    print(\"Run shell command:\\n\\t\", cmd)\n",
    "    return subprocess.run(cmd, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57214c01-5b32-47ca-bdc8-76724431b7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEA\n",
    "bea_cmd = \"\"\"\n",
    "docker run --rm  -v /mnt/green-efs/kostiantyn.omelianchuk/gec_sota/data/evalsets/bea-dev.txt:/data/input.txt \\\n",
    "    -v /mnt/green-efs/kostiantyn.omelianchuk/gec_sota/data/evalsets/bea-dev.m2:/data/ref.m2 \\\n",
    "    -v \"{pred_txt}\":/data/pred.txt \\\n",
    "    errant\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd19b684-226c-4192-b04e-185b2ad9f4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_path = '/home/oleksandr.korniienko/LLaMA-Factory/saves/LLaMA2-7B-gec-greco/dpo-02'\n",
    "# experiment_path = '/home/oleksandr.korniienko/LLaMA-Factory/saves/LLaMA2-7B-gec-full'\n",
    "\n",
    "\n",
    "experiments = [\n",
    "    '/home/oleksandr.korniienko/LLaMA-Factory/saves/LLaMA2-7B-gec-greco/dpo-02',\n",
    "    '/home/oleksandr.korniienko/LLaMA-Factory/saves/LLaMA2-7B-gec-greco/dpo-05',\n",
    "    '/home/oleksandr.korniienko/LLaMA-Factory/saves/LLaMA2-7B-gec-greco/dpo-07',\n",
    "    '/home/oleksandr.korniienko/LLaMA-Factory/saves/LLaMA2-7B-gec-greco/dpo-1',\n",
    "    '/home/oleksandr.korniienko/LLaMA-Factory/saves/LLaMA2-7B-gec-greco/dpo-4'\n",
    "]\n",
    "\n",
    "\n",
    "for experiment_path in experiments:\n",
    "    print(\"==================\")\n",
    "    print(experiment_path)\n",
    "\n",
    "    output_path = os.path.join(experiment_path, 'predict-nucle', 'predictions.csv')\n",
    "\n",
    "    with open(os.path.join(experiment_path, 'predict-nucle', 'generated_predictions.jsonl'), 'r') as f:\n",
    "        df = pd.DataFrame( [json.loads(l) for l in f.readlines()] )\n",
    "    df['output'] = df['predict']\n",
    "    df['output'] = df.output.map(tokenize)\n",
    "    df.to_csv(output_path, index=False)\n",
    "        \n",
    "    cmd_evaluate = f's2s_evaluate --eval /mnt/green-efs/kostiantyn.omelianchuk/data/evalsets/nucle14-2a.m2 --pred {output_path} --out {experiment_path}/metrics-nucle.yaml  --metrics max_match wed --print-format yaml'\n",
    "    \n",
    "    run(cmd_evaluate)\n",
    "\n",
    "    ######\n",
    "    output_path = os.path.join(experiment_path, 'predict-bea', 'predictions.txt')\n",
    "    \n",
    "    with open(os.path.join(experiment_path, 'predict-bea', 'generated_predictions.jsonl'), 'r') as f:\n",
    "        df = pd.DataFrame( [json.loads(l) for l in f.readlines()] )\n",
    "    df['output'] = df['predict']\n",
    "    df['output'] = df.output.map(tokenize)\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(\"\\n\".join(df.output.tolist()))\n",
    "    run(bea_cmd.format(pred_txt=output_path))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
